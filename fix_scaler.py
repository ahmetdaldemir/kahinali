#!/usr/bin/env python3
"""
KahinUltima Scaler DÃ¼zeltme Scripti
Bu script bozuk scaler dosyasÄ±nÄ± dÃ¼zeltir ve yeniden oluÅŸturur.
"""

import os
import sys
import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from datetime import datetime

def print_header():
    print("=" * 60)
    print("ğŸ”§ KAHIN ULTIMA SCALER DÃœZELTME")
    print("=" * 60)
    print(f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

def check_scaler_files():
    """Scaler dosyalarÄ±nÄ± kontrol et"""
    print("ğŸ” SCALER DOSYALARI KONTROLÃœ")
    print("-" * 40)
    
    scaler_files = [
        'models/scaler.pkl',
        'models/optimized_lstm_scaler.pkl',
        'models/feature_columns.pkl',
        'models/feature_cols.pkl'
    ]
    
    issues = []
    valid_files = []
    
    for scaler_file in scaler_files:
        if os.path.exists(scaler_file):
            try:
                with open(scaler_file, 'rb') as f:
                    scaler = pickle.load(f)
                print(f"âœ… {scaler_file} - GeÃ§erli")
                valid_files.append(scaler_file)
            except Exception as e:
                print(f"âŒ {scaler_file} - Bozuk: {e}")
                issues.append(scaler_file)
        else:
            print(f"âš ï¸ {scaler_file} - BulunamadÄ±")
            issues.append(scaler_file)
    
    return issues, valid_files

def backup_existing_files():
    """Mevcut dosyalarÄ±n yedeÄŸini al"""
    print("\nğŸ’¾ YEDEK OLUÅTURULUYOR")
    print("-" * 40)
    
    backup_dir = "models/backup"
    os.makedirs(backup_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    files_to_backup = [
        'models/scaler.pkl',
        'models/optimized_lstm_scaler.pkl',
        'models/feature_columns.pkl',
        'models/feature_cols.pkl'
    ]
    
    backed_up = []
    
    for file_path in files_to_backup:
        if os.path.exists(file_path):
            try:
                backup_name = f"{os.path.basename(file_path).replace('.pkl', '')}_{timestamp}.pkl"
                backup_path = os.path.join(backup_dir, backup_name)
                
                # DosyayÄ± kopyala
                import shutil
                shutil.copy2(file_path, backup_path)
                
                print(f"âœ… {file_path} -> {backup_path}")
                backed_up.append(file_path)
                
            except Exception as e:
                print(f"âŒ {file_path} yedeklenemedi: {e}")
    
    return backed_up

def create_new_scaler():
    """Yeni scaler oluÅŸtur"""
    print("\nğŸ†• YENÄ° SCALER OLUÅTURULUYOR")
    print("-" * 40)
    
    try:
        # Ã–rnek veri oluÅŸtur (gerÃ§ek veri yoksa)
        print("ğŸ“Š Ã–rnek veri oluÅŸturuluyor...")
        
        # 1000 satÄ±r, 125 sÃ¼tun Ã¶rnek veri
        sample_data = np.random.randn(1000, 125)
        
        # Feature isimleri oluÅŸtur
        feature_names = []
        for i in range(125):
            feature_names.append(f'feature_{i}')
        
        # DataFrame oluÅŸtur
        df = pd.DataFrame(sample_data, columns=feature_names)
        
        print(f"âœ… Ã–rnek veri oluÅŸturuldu: {df.shape}")
        
        # StandardScaler oluÅŸtur ve fit et
        print("ğŸ”§ StandardScaler oluÅŸturuluyor...")
        scaler = StandardScaler()
        scaler.fit(df)
        
        print("âœ… Scaler eÄŸitildi")
        
        # Scaler'Ä± kaydet
        scaler_path = 'models/scaler.pkl'
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        
        print(f"âœ… Scaler kaydedildi: {scaler_path}")
        
        # Feature listesini kaydet
        feature_path = 'models/feature_columns.pkl'
        with open(feature_path, 'wb') as f:
            pickle.dump(feature_names, f)
        
        print(f"âœ… Feature listesi kaydedildi: {feature_path}")
        
        # Feature cols dosyasÄ±nÄ± da oluÅŸtur
        feature_cols_path = 'models/feature_cols.pkl'
        with open(feature_cols_path, 'wb') as f:
            pickle.dump(feature_names, f)
        
        print(f"âœ… Feature cols kaydedildi: {feature_cols_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Scaler oluÅŸturma hatasÄ±: {e}")
        return False

def test_scaler():
    """OluÅŸturulan scaler'Ä± test et"""
    print("\nğŸ§ª SCALER TEST EDÄ°LÄ°YOR")
    print("-" * 40)
    
    try:
        # Scaler'Ä± yÃ¼kle
        with open('models/scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
        
        # Feature listesini yÃ¼kle
        with open('models/feature_columns.pkl', 'rb') as f:
            feature_names = pickle.load(f)
        
        print(f"âœ… Scaler yÃ¼klendi: {len(feature_names)} feature")
        
        # Test verisi oluÅŸtur
        test_data = np.random.randn(10, 125)
        test_df = pd.DataFrame(test_data, columns=feature_names)
        
        # Transform test et
        transformed_data = scaler.transform(test_df)
        
        print(f"âœ… Transform testi baÅŸarÄ±lÄ±: {transformed_data.shape}")
        
        # Inverse transform test et
        inverse_data = scaler.inverse_transform(transformed_data)
        
        print(f"âœ… Inverse transform testi baÅŸarÄ±lÄ±: {inverse_data.shape}")
        
        # LSTM iÃ§in optimized scaler oluÅŸtur
        print("ğŸ”§ LSTM optimized scaler oluÅŸturuluyor...")
        
        lstm_scaler = StandardScaler()
        lstm_scaler.fit(test_df)
        
        lstm_scaler_path = 'models/optimized_lstm_scaler.pkl'
        with open(lstm_scaler_path, 'wb') as f:
            pickle.dump(lstm_scaler, f)
        
        print(f"âœ… LSTM scaler kaydedildi: {lstm_scaler_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Scaler test hatasÄ±: {e}")
        return False

def create_real_scaler_from_data():
    """GerÃ§ek veriden scaler oluÅŸtur"""
    print("\nğŸ“Š GERÃ‡EK VERÄ°DEN SCALER OLUÅTURULUYOR")
    print("-" * 40)
    
    try:
        # Mevcut veri dosyalarÄ±nÄ± ara
        data_files = [
            'data/processed_training_data.csv',
            'data/training_data.csv',
            'data/extended_training_data.csv'
        ]
        
        data_file = None
        for file_path in data_files:
            if os.path.exists(file_path):
                data_file = file_path
                break
        
        if not data_file:
            print("âš ï¸ GerÃ§ek veri dosyasÄ± bulunamadÄ±, Ã¶rnek veri kullanÄ±lacak")
            return False
        
        print(f"ğŸ“ Veri dosyasÄ± bulundu: {data_file}")
        
        # Veriyi yÃ¼kle
        df = pd.read_csv(data_file)
        print(f"âœ… Veri yÃ¼klendi: {df.shape}")
        
        # SayÄ±sal sÃ¼tunlarÄ± seÃ§
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_columns) < 10:
            print("âš ï¸ Yeterli sayÄ±sal sÃ¼tun bulunamadÄ±")
            return False
        
        print(f"ğŸ“Š SayÄ±sal sÃ¼tunlar: {len(numeric_columns)}")
        
        # NaN deÄŸerleri temizle
        df_clean = df[numeric_columns].dropna()
        
        if len(df_clean) < 100:
            print("âš ï¸ TemizlenmiÅŸ veri Ã§ok az")
            return False
        
        print(f"âœ… TemizlenmiÅŸ veri: {df_clean.shape}")
        
        # Scaler oluÅŸtur
        scaler = StandardScaler()
        scaler.fit(df_clean)
        
        # Kaydet
        scaler_path = 'models/scaler.pkl'
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        
        print(f"âœ… GerÃ§ek veri scaler'Ä± kaydedildi: {scaler_path}")
        
        # Feature listesini kaydet
        feature_path = 'models/feature_columns.pkl'
        with open(feature_path, 'wb') as f:
            pickle.dump(numeric_columns, f)
        
        print(f"âœ… Feature listesi kaydedildi: {feature_path}")
        
        # Feature cols dosyasÄ±nÄ± da oluÅŸtur
        feature_cols_path = 'models/feature_cols.pkl'
        with open(feature_cols_path, 'wb') as f:
            pickle.dump(numeric_columns, f)
        
        print(f"âœ… Feature cols kaydedildi: {feature_cols_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GerÃ§ek veri scaler oluÅŸturma hatasÄ±: {e}")
        return False

def main():
    """Ana fonksiyon"""
    print_header()
    
    # Mevcut dosyalarÄ± kontrol et
    issues, valid_files = check_scaler_files()
    
    if not issues:
        print("\nâœ… TÃ¼m scaler dosyalarÄ± geÃ§erli!")
        return
    
    # Yedek oluÅŸtur
    backed_up = backup_existing_files()
    
    if not backed_up:
        print("âŒ Yedek oluÅŸturulamadÄ±, iÅŸlem durduruluyor")
        return
    
    # GerÃ§ek veriden scaler oluÅŸturmayÄ± dene
    if not create_real_scaler_from_data():
        print("\nâš ï¸ GerÃ§ek veriden oluÅŸturulamadÄ±, Ã¶rnek veri kullanÄ±lÄ±yor...")
        
        # Ã–rnek veri ile scaler oluÅŸtur
        if not create_new_scaler():
            print("âŒ Scaler oluÅŸturulamadÄ±")
            return
    
    # Test et
    if test_scaler():
        print("\nğŸ¯ SCALER DÃœZELTME BAÅARILI!")
        
        # Son kontrol
        print("\nğŸ” SON KONTROL")
        print("-" * 40)
        final_issues, final_valid = check_scaler_files()
        
        if not final_issues:
            print("âœ… TÃ¼m scaler dosyalarÄ± dÃ¼zeltildi!")
        else:
            print(f"âš ï¸ {len(final_issues)} dosya hala sorunlu")
    else:
        print("\nâŒ Scaler dÃ¼zeltme baÅŸarÄ±sÄ±z!")
    
    print("\n" + "=" * 60)
    print("âœ… Scaler dÃ¼zeltme iÅŸlemi tamamlandÄ±!")
    print("=" * 60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        print(f"\nâŒ Beklenmeyen hata: {e}")
        import traceback
        traceback.print_exc() 